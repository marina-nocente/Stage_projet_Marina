####### Snakefile Marina stage IGR septembre 2020 : analyses de ChIP-seq #######

"""
Script Python à mettre avant la rule all pour interpréter les samples dans le fichier de config
et lister les echantillons.
"""

import os.path as op # permet de faciliter l acces aux chemins de fichier etc
import sys # permet des interactions avec le systeme

# pour appeler le fichier config
configfile: "./config.yaml"

#print(config["samples"]["Oct4_rep1"]["r1"]) # print le fichier "ChIPseq_Oct4_rep1_1.fastq.gz"
#print(config["samples"].keys()) # affiche les cles du dictionnaire "samples"


## Faire une liste de mes fastq files à partir de ma liste d echantillons
list_sample = ['Oct4_rep1', 'Oct4_rep2', 'TBP_rep1', 'TBP_rep2', 'Pol2_rep1', 'Pol2_rep2', 'CTCF', 'Chd8_rep1', 'Chd8_rep2']
STREAM = ["1", "2"]

list_fastq = []

for ech in list_sample:
    #print(ech)
    for sens in STREAM:
        list_fastq.append("ChIPseq_" + ech + "_" + sens)
#print(list_fastq)

## on peut enchainer le nom des fastq file avec des pipes de la facon suivante :
fastq_wildcard_constraints = "|".join(list_fastq)
#print(fastq_wildcard_constraints)


## Faire une liste de mes samples files à partir de ma liste d'echantillons

list_ChIP = []

for ech in list_sample:
    #print(ech)
    list_ChIP.append("ChIPseq_" + ech)
#print(list_ChIP)

## on peut enchainer le nom des samples file avec des pipes de la facon suivante :
sample_wildcard_constraints = "|".join(list_ChIP)
#print(sample_wildcard_constraints)


## On ne fait pas confiance aux ordis! On precise le nom des fichiers
wildcard_constraints:
    fastq_wildcard = fastq_wildcard_constraints,
    sample_wildcard = sample_wildcard_constraints


# il faut faire des noms de fichiers uniformes, c est plus facile
# pour liste les echantillons, on commence par des guillemets doubles, puis on ecrit les echantillons separes par des pipes mais SANS espace.


## On peut faire un raport final:
report: "report_analyse_ChIP-seq_Marina.rst"


#############################################################


rule all:
    input:
#        expand(op.join(config["outputdir"], "fastQC", "{fastq_wildcard}_fastqc.html"), fastq_wildcard = list_fastq),
#        expand(op.join(config["outputdir"], "fastQC", "{fastq_wildcard}_fastqc.zip"), fastq_wildcard = list_fastq),
#        expand(op.join(config["outputdir"], "fastp", "cleaned_filtered_{sample_wildcard}_1.fastq.gz"), sample_wildcard = list_ChIP),
#        expand(op.join(config["outputdir"], "fastp", "cleaned_filtered_{sample_wildcard}_2.fastq.gz"), sample_wildcard = list_ChIP),
#        expand(op.join(config["outputdir"], "fastp", "failed_out_{sample_wildcard}.txt"), sample_wildcard = list_ChIP),
#        expand(op.join(config["outputdir"], "fastp", "fastp_{sample_wildcard}.json"), sample_wildcard = list_ChIP),
#        expand(op.join(config["outputdir"], "fastp", "fastp_{sample_wildcard}.html"), sample_wildcard = list_ChIP),
        op.join(config["outputdir"], "multiQC", "multiqc.html"),
#        expand(op.join(config["outputdir"], "bowtie2", "{sample_wildcard}.bam"), sample_wildcard = list_ChIP),
#        expand(op.join(config["outputdir"],  "samtools_sort", "{sample_wildcard}.sorted.bam"), sample_wildcard = list_ChIP),
        expand("results/picard/deduplicate_{sample_wildcard}.bam", sample_wildcard = list_ChIP),
        expand("results/picard/deduplicate_{sample_wildcard}.bam.bai", sample_wildcard = list_ChIP),
        expand("results/macs2/{sample_wildcard}_peaks.narrowPeak", sample_wildcard = list_ChIP),
        expand("results/macs2/{sample_wildcard}_summits.bed", sample_wildcard = list_ChIP),
        expand("results/macs2/{sample_wildcard}_treat_pileup.bdg", sample_wildcard = list_ChIP),
        expand("results/bigwig/{sample_wildcard}.bw", sample_wildcard = list_ChIP),
        expand("results/macs2/just_chr_{sample_wildcard}_peaks.narrowPeak", sample_wildcard = list_ChIP)


#       expand(op.join(config["outputdir"], "QC", "fastqScreen1", "{sample_wildcard}_screen.html"), sample_wildcard=Fastq_name_liste),
#       expand(op.join(config["outputdir"], "QC", "fastqScreen1", "{sample_wildcard}_screen.txt"), sample_wildcard=Fastq_name_liste),

#        expand(op.join(config["outputdir"], "samtools_flagstat", "{sample_wildcard}.bam.flagstat"), sample_wildcard = list_ChIP)



##############################################################


"""
Cette regle realise l'analyse FastQC de mes fichiers fastq.gz avant trimming
2/4g
25/30min
"""
rule fastqc:
    input:
        op.join(config["datadir"], "{fastq_wildcard}.fastq.gz")
    output:
        html=protected(report(op.join(config["outputdir"], "fastQC", "{fastq_wildcard}_fastqc.html"), category="FastQC Reports", caption="fastqc.rst")),
        zip=protected(report(op.join(config["outputdir"], "fastQC", "{fastq_wildcard}_fastqc.zip"), category="FastQC Reports", caption="fastqc.rst"))
    message:
        "On controle la qualite avant trimming de {input}"
    threads:
        config["fastqc"]["threads"]
    resources:
        mem_mb = lambda wildcards, attempt: min((1024 * 2) + (2 * attempt), 6 * 1024),
        time_min = lambda wildcards, attempt: 25 + (15 * attempt)
    log:
        op.join(config["outputdir"], "fastQC", "logs", "{fastq_wildcard}.log")
    params:
	""
    wrapper:
        "Unofficial/bio/fastqc" # le numero des wrappers doivent être mis à jour quand l'outil est lui meme mis a jour



"""
Cette regle realise le trimming de mes echantillons fastq.gz pour eliminer les adaptateurs avec fastp
2/4g
15/30m
"""
rule fastp:
    input:
        sample=["raw_data/{sample_wildcard}_1.fastq.gz", "raw_data/{sample_wildcard}_2.fastq.gz"]
    output:
        trimmed=protected(["results/fastp/cleaned_filtered_{sample_wildcard}_1.fastq.gz", "results/fastp/cleaned_filtered_{sample_wildcard}_2.fastq.gz"]),
        html=protected(report("results/fastp/{sample_wildcard}.html", category="Fastp Reports", caption="fastp.rst")),
        json=protected(report("results/fastp/{sample_wildcard}.json", category="Fastp Reports", caption="fastp.rst"))
    message:
        "Etape de trimming : on elimine les adaptateurs de {input} et on nettoie avec fastp"
    resources:
        mem_mb = lambda wildcards, attempt: min((1024 * 2) + (2 * attempt), 6 * 1024),
        time_min = lambda wildcards, attempt: 15 + (15 * attempt)
    threads: 20
    log:
        op.join(config["outputdir"], "fastp", "logs", "{sample_wildcard}.log")
    params:
        extra="--length_required 25 --detect_adapter_for_pe"
    wrapper:
        "Unofficial/bio/fastp"



"""
Cette regle telecharge le genome de la souris
128/512mb
2/4h
"""
rule download_fasta:
    output:
        protected(op.join(config["datadir"], "Genome_souris", "GRCm38.primary_assembly.genome.fa"))
    params:
        species = "mus_musculus",
        datatype = "dna",
        build = "GRCm38",
        release = "99"
    log:
        "logs/download/GRCm38.99.log"
    threads: 1
    resources:
        mem_mb = lambda wildcards, attempt: min(128 + (512 * attempt), 1024),
        time_min = lambda wildcards, attempt: 120 + (120 * attempt)
    wrapper:
        "Unofficial/bio/reference/ensembl-sequence"


"""
Cette regle realise l'indexation du genome de la souris avec bowtie2
5/7gb
1/2h
"""
rule bowtie2_index:
    input:
        fasta = op.join(config["datadir"], "Genome_souris", "GRCm38.primary_assembly.genome.fa")
    output:
        protected(op.join(config["outputdir"], "mouse_index/mouse_genome_mm10.1.bt2"))
    message:
        "On indexe le genome de {input} avec bowtie2"
    threads:
        config["bowtie2"]["threads"]
    log:
        op.join(config["outputdir"], "mouse_index", "logs", "GRCm38.primary_assembly.genome.log")
    resources:
        mem_mb = lambda wildcards, attempt: min((1024 * 5) + (2 * attempt), 10 * 1024),
        time_min = lambda wildcards, attempt: 60 + (60 * attempt)
    params:
        prefix="{}/mouse_index/mouse_genome_mm10".format(config["outputdir"]) #mouse_genome_mm10 est mon prefixe
    wrapper:
        "Unofficial/bio/bowtie2/build" # le numero des wrappers doivent être mis à jour quand l'outil est lui meme mis a jour




"""
Cette regle realise le mapping des reads sur le genome de la souris indexe avec bowtie2
8/10gb
45/60min
"""
rule bowtie2_mapping:
    input:
        sample=["results/fastp/cleaned_filtered_{sample_wildcard}_1.fastq.gz", "results/fastp/cleaned_filtered_{sample_wildcard}_2.fastq.gz"],
        index = op.join(config["outputdir"], "mouse_index/mouse_genome_mm10.1.bt2")
    output:
        protected(report(op.join(config["outputdir"], "bowtie2", "{sample_wildcard}.bam"), category="bowtie2 Reports", caption="bowtie2.rst"))
    message:
        "On mappe les reads sur le genome avec bowtie2"
    priority: 200
    threads:
        config["bowtie2"]["threads"] # Use at least two threads
    log:
        op.join(config["outputdir"], "bowtie2", "logs", "{sample_wildcard}.log")
    resources:
        mem_mb = lambda wildcards, attempt: min((1024 * 8) + (2 * attempt), 10 * 1024),
        time_min = lambda wildcards, attempt: 140 + (120 * attempt)
    params:
        index="{}/mouse_index/mouse_genome_mm10".format(config["outputdir"]), # prefix of reference genome index (built with bowtie2-build)
        extra="" #  "--align-paired-reads"
    wrapper:
        "Unofficial/bio/bowtie2/align"




"""
Cette regle permet d'obtenir des statistiques sur le mapping des reads avec samtools_flagstat
1/1.5gb
10/30min
"""

rule samtools_flagstat:
    input:
        op.join(config["outputdir"],  "bowtie2", "{sample_wildcard}.bam")
    output:
        protected(report(op.join(config["outputdir"], "samtools_flagstat", "{sample_wildcard}.bam.flagstat"), category="samtools_flagstat Reports", caption="samtools_flagstat.rst"))
    threads: 1
    resources:
        mem_mb = lambda wildcards, attempt: min((1024 * 1) + (1 * attempt), 2 * 1024),
        time_min = lambda wildcards, attempt: 10 + (20 * attempt)
    wrapper:
        "Unofficial/bio/samtools/flagstat"



"""
Cette regle permet de "resumer" les resultats de fastp grace a multiQC
1/3g
30/2h
"""
rule multiQC:
    input:
        expand("results/fastp/cleaned_filtered_{list_fastq}.fastq.gz", list_fastq=list_fastq), # ["Oct4_rep1_1", "Oct4_rep1_2", "Oct4_rep2_1", "Oct4_rep2_1","TBP_rep1_1", "TBP_rep1_2", "TBP_rep2_1", "TBP_rep2_2", "Pol2_rep1_1", "Pol2_rep1_2","Pol2_rep2_1", "Pol2_rep2_2", "CTCF_1", "CTCF_2", "Chd8_rep1_1", "Chd8_rep1_2", "Chd8_rep2_1", "Chd8_rep2_2"]),
        expand("results/fastQC/{list_fastq}_fastqc.zip", list_fastq=list_fastq), # ["Oct4_rep1_1", "Oct4_rep1_2", "Oct4_rep2_1", "Oct4_rep2_1","TBP_rep1_1", "TBP_rep1_2", "TBP_rep2_1", "TBP_rep2_2", "Pol2_rep1_1", "Pol2_rep1_2","Pol2_rep2_1", "Pol2_rep2_2", "CTCF_1", "CTCF_2", "Chd8_rep1_1", "Chd8_rep1_2", "Chd8_rep2_1", "Chd8_rep2_2"]),
        expand("results/samtools_flagstat/ChIPseq_{list_sample}.bam.flagstat", list_sample=list_sample), # ['Oct4_rep1', 'Oct4_rep2', 'TBP_rep1', 'TBP_rep2', 'Pol2_rep1', 'Pol2_rep2', 'CTCF', 'Chd8_rep1', 'Chd8_rep2']),
        expand("results/picard/deduplicate_ChIPseq_{list_sample}.metrics.txt", list_sample=list_sample), # ['Oct4_rep1', 'Oct4_rep2', 'TBP_rep1', 'TBP_rep2', 'Pol2_rep1', 'Pol2_rep2', 'CTCF', 'Chd8_rep1', 'Chd8_rep2']),
        expand("results/samtools_flagstat/deduplicate_ChIPseq_{list_sample}.bam.flagstat", list_sample=list_sample), # ['Oct4_rep1', 'Oct4_rep2', 'TBP_rep1', 'TBP_rep2', 'Pol2_rep1', 'Pol2_rep2', 'CTCF', 'Chd8_rep1', 'Chd8_rep2'])
        expand("results/macs2/ChIPseq_{list_sample}_peaks.xls", list_sample=list_sample)
    output:
        protected(report(op.join(config["outputdir"], "multiQC", "multiqc.html"), category="multiQC Reports", caption="multiqc.rst"))
    message:
        "MulitQC permet de présenter un resume des resultats de fastp sur {input}"
    log:
        op.join(config["outputdir"], "multiQC", "logs", "multiqc.log")
    threads: 1
    resources:
        mem_mb = lambda wildcards, attempt: min((1024 * 1) + (2 * attempt), 4 * 1024),
        time_min = lambda wildcards, attempt: 30 + (60 * attempt)
    wrapper:
        "Unofficial/bio/multiqc"



# """
# Cette regle realise l'analyse FastqScreen de mes fichiers fastq.gz avant trimming
# """
# rule fastqScreen:
#     input:
#         r = op.join(config["datadir"], "{Fastq_name_liste}.fastq.gz")
#     output:
#         report(op.join(config["outputdir"], "fastqScreen", "{Fastq_name_liste}_screen.html"), category="FastqScreen Reports", caption="fastqScreen.rst"),
#         report(op.join(config["outputdir"], "fastqScreen", "{Fastq_name_liste}_screen.txt"), category="FastqScreen Reports", caption="fastqScreen.rst")
#     message:
#         "fastqscreen avant trimming de {input}"
#     conda:
#         "fastqScreen.yaml"
#     threads:
#         config["fastqscreen"]["threads"]
#     log:
#         op.join(config["outputdir"], "fastqScreen", "logs", "{Fastq_name_liste}.log")
#     params:
#         outdir = op.join(config["outputdir"], "fastqScreen"),
#         conf = config["fastqscreen"]["config"],
#         aligner = config["fastqscreen"]["aligner"]
#     shell:
#         "fastq_screen {input.r} --outdir {params.outdir} --threads {threads} --aligner {params.aligner}  --conf {params.conf} &> {log}"
#



"""
Cette regle permet de trier les fichiers bam en fonction de leurs coordonnees grace a samtools samtools_sort
10gb
20/45min
"""
#def my_lambda(wildcards, attempt):
#    return 45 + 15 * attempt

rule samtools_sort:
    input:
        op.join(config["outputdir"],  "bowtie2", "{sample_wildcard}.bam")
    output:
        protected(report(op.join(config["outputdir"],  "samtools_sort", "{sample_wildcard}.sorted.bam"), category="Samtools sort", caption="samtools_sort.rst"))
    resources:
        mem_mb = 1024 * 10,
        time_min = lambda wildcards, attempt: 45 + (15 * attempt)
    params:
        " -m 756M " # Approximately the maximum required memory per thread. To prevent sort from creating a huge number of temporary files, it enforces a minimum value of 1M for this setting.
    threads:  # Samtools takes additional threads through its option -@. One thread for samtools. Other threads are *additional* threads passed to the argument -@.
        10     # This value - 1 will be sent to -@.
    wrapper:
        "Unofficial/bio/samtools/sort"




"""
Cette regle permet de marquer les reads dupliques grace a picard
5/8gb,
45/60min
"""

rule mark_duplicates:
    input:
        op.join(config["outputdir"],  "samtools_sort", "{sample_wildcard}.sorted.bam")
    output:
        bam=protected(report("results/picard/deduplicate_{sample_wildcard}.bam", category="Picard Reports", caption="picard.rst")),
        metrics=protected(report("results/picard/deduplicate_{sample_wildcard}.metrics.txt", category="Picard Reports", caption="picard.rst"))
    log:
        "results/picard/logs/deduplicate_{sample_wildcard}.log"
    resources:
        mem_mb = lambda wildcards, attempt: min((1024 * 7) + (3 * attempt), 10 * 1024),
        time_min = lambda wildcards, attempt: 45 + (15 * attempt)
    threads: 1
    params:
        "-Djava.io.tmpdir=javatmp_{sample_wildcard} TMP_DIR=picardtmp_{sample_wildcard} REMOVE_DUPLICATES=true VALIDATION_STRINGENCY=SILENT " # If true do not write duplicates to the output file instead of writing them with appropriate flags set.
    wrapper:
        "Unofficial/bio/picard/markduplicates"



"""
Cette regle permet de verifier que l'on a recupere uniquement les flags des reads uniques bien mappes apres picard
1.5/3gb
10/15min
"""

rule samtools_flagstat_picard:
    input:
        op.join(config["outputdir"], "picard", "deduplicate_{sample_wildcard}.bam")
    output:
        protected(report(op.join(config["outputdir"], "samtools_flagstat", "deduplicate_{sample_wildcard}.bam.flagstat"), category="samtools_flagstat_apres_Picard Reports", caption="samtools_flagstat.rst"))
    resources:
        mem_mb = lambda wildcards, attempt: min((1024 * 1) + (2 * attempt), 4 * 1024),
        time_min = lambda wildcards, attempt: 10 + (10 * attempt)
    threads: 1
    wrapper:
        "Unofficial/bio/samtools/flagstat"




"""
Cette regle permet d indexer les bam filtres grace a samtools_index pour visualiser dans IGV
1/2gb
5/20min
"""

rule samtools_index:
    input:
        "results/picard/deduplicate_{sample_wildcard}.bam"
    output:
        protected(report("results/picard/deduplicate_{sample_wildcard}.bam.bai", category="samtools index", caption="samtools_index.rst"))
    resources:
        mem_mb = lambda wildcards, attempt: min((1024 * 1) + (1 * attempt), 3 * 1024),
        time_min = lambda wildcards, attempt: 5 + (15 * attempt)
    threads: 1
    wrapper:
        "Unofficial/bio/samtools/index"



"""
Cette regle permet de faire du peak-calling avec MACS2
8/10gb
60min
"""

rule callpeak:
    input:
        treatment="results/picard/deduplicate_{sample_wildcard}.bam",   # required: treatment sample(s)
        #control=""      # optional: control sample(s)
    output:
        # all output-files must share the same basename and only differ by it's extension
        # Usable extensions (and which tools they implicitly call) are listed here:
        #         https://snakemake-wrappers.readthedocs.io/en/stable/wrappers/macs2/callpeak.html.
        (report(multiext("results/macs2/{sample_wildcard}",
                        "_peaks.xls",   ### required, a table with information about called peaks
                        ### optional output files
                        "_peaks.narrowPeak", # contains the peak locations, peak summit, p-value and q-value
                        "_summits.bed", # eak summits locations for every peak
                        ### these output extensions internally set the --bdg or -B option:
                        "_treat_pileup.bdg" # pileup signals from treatment sample
                        ), category="MACS2", caption="macs2.rst"))
    log:
        "results/macs2/callpeak_{sample_wildcard}.log"
    resources:
        mem_mb = lambda wildcards, attempt: min((1024 * 8) + (2 * attempt), 10 * 1024),
        time_min = lambda wildcards, attempt: 60 + (60 * attempt)
    threads: 1
    params:
        "-f BAM -g mm --nomodel"
        # -f : Format of tag file is bam,
        # -g : It's the mappable genome size or effective genome size which is defined as the genome size which can be sequenced.
        # --nomodel : While on, MACS will bypass building the shifting model.
    wrapper:
        "Unofficial/bio/macs2/callpeak"


"""
Cette regle permet de convertir mes fichiers bam en bigwig pour visualiser sous IGV
"""

rule test_deeptools_bamcoverage:
    input:
        bam = "results/picard/deduplicate_{sample_wildcard}.bam"
    output:
        coverage = "results/bigwig/{sample_wildcard}.bw"
    message:
        "Testing deeptools bamcoverage"
    threads:
        1
    resources:
        mem_mb = (
            lambda wildcards, attempt: min(attempt * 1024, 10240)
        ),
        time_min = (
            lambda wildcards, attempt: min(attempt * 20, 200)
        )
    log:
        "results/bigwig/logs/{sample_wildcard}_test_deeptools_bamcoverage.log"
    wrapper:
        "Unofficial/bio/deeptools/bamcoverage"


"""
Cette regle permet de supprimer les peaks qui sont au niveau de region contig sur le genome
"""

rule suppression_contig:
    input:
        "results/macs2/{sample_wildcard}_peaks.narrowPeak"
    output:
        "results/macs2/just_chr_{sample_wildcard}_peaks.narrowPeak"
    message:
        "Suppression des peaks tombant dans les regions contigs"
    threads:
        1
    resources:
        mem_mb = (
            lambda wildcards, attempt: min(attempt * 1024, 10240)
        ),
        time_min = (
            lambda wildcards, attempt: min(attempt * 20, 200)
        )
    shell:
        'grep "chr" {input} > {output}'
